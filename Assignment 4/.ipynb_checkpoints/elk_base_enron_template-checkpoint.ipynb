{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea4f66-2eb4-495f-9d43-f908d9b8bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6976255c-94a6-4011-a5ac-9f525e50b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0335521e-0866-4d4c-b460-501779bdbfd5",
   "metadata": {},
   "source": [
    "## Setting up the Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85059e35-63c5-459c-a6e9-af73ef07de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "ELASTIC_PASSWORD = \"p2iFCHUbC7ze1QoIMVw\"\n",
    "\n",
    "es = Elasticsearch(\"http://elasticsearch:9200\",\n",
    "                    basic_auth=(\"elastic\", ELASTIC_PASSWORD))\n",
    "\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ceb822-d6d1-43ba-b2e2-dfb31c297952",
   "metadata": {},
   "source": [
    "## Setting up the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2abdb4-a966-43de-bbc6-d32041c1f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the data\n",
    "\"\"\"\n",
    "The data is uploaded at a google drive directory, by running this code box, \n",
    "the data will be downloaded and you will have access to it at\n",
    "(enron_short/maildir)\n",
    "\"\"\"\n",
    "import gdown\n",
    "\n",
    "googleDriveURL = \"https://drive.google.com/file/d/10OwgK91e0lNRsrAV31sSJu7KJUujBTnE/view?usp=sharing\"\n",
    "output = 'enron_short.tar.gz'\n",
    "gdown.download(googleDriveURL, output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e9a89b-1b35-44ea-8b64-331b365f4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the directory exists\n",
    "from os import path\n",
    "\n",
    "zipfile_path = \"enron_short.tar.gz\"\n",
    "if not path.isfile(zipfile_path):\n",
    "    print(\"the zipfile is not here, please ensure you download it first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ed7b9-b2e4-46e7-9c2f-54679c3f9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xf enron_short.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747eb047-8aab-47c4-b244-ee73eb984930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the directory exists\n",
    "from os import path\n",
    "\n",
    "documents_path = r\"enron_short/maildir\"\n",
    "if not path.exists(documents_path):\n",
    "    print(\"the directory is not here, please ensure you have the documnets first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4210f-ede3-476d-82e3-bad55c8c1b9b",
   "metadata": {},
   "source": [
    "## Your Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ce99f5-6c1c-4bb2-81f2-a315331b007b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (493592574.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    stop_words = set(stopwords.words('english'));\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "# b) Indexer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Tokenize a document and give out a set without stopwords\n",
    "def tokenize(document): {\n",
    "    #Just a list of all stop words\n",
    "    stop_words = set(stopwords.words('english'));\n",
    "\n",
    "    # Tokenize or split into a list of words\n",
    "    word_tokens = word_tokenize(file);\n",
    "    token_set = set(word_tokens)\n",
    "\n",
    "    # Adds word to list as lowercase, if not in stop_words\n",
    "    filtered_sentence = [ w for w in token_set if not w.lower in stop_words ]\n",
    "\n",
    "    return filtered_sentence\n",
    "}\n",
    "\n",
    "# Take list of documents and gives out tokenized list with the inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "\n",
    "    # Enumerate gives an index and the value\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        tokens = tokenize(document)\n",
    "        \n",
    "        for token in tokens: \n",
    "            inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Writes a file with the terms of the inverted indexes\n",
    "def save_index(index, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for term, postings in index.items():\n",
    "            file.write(f\"{term}: {', '.join(map(str, postings))}\\n\")\n",
    "\n",
    "\n",
    "def walk_os(input_folder):\n",
    "    for root, dirs, files in os.walk(input_folder):\n",
    "        relative_root = Path(root).relative_to(input_folder)\n",
    "        files = map(lambda p: Path(relative_root, p), files)\n",
    "        files = filter(lambda f: f.suffix == \".txt\", files)\n",
    "        yield files\n",
    "\n",
    "\n",
    "def index(input_folder):\n",
    "    # Gives back the files\n",
    "    files = walk_os(input_folder)\n",
    "\n",
    "    # Tokenize and invert indexes\n",
    "    index = build_inverted_index(files)\n",
    "\n",
    "    # Save the indexes to a file for easy access\n",
    "    save_index(index, 'output.txt')\n",
    "\n",
    "    # Return the inverted index\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967152ef-c086-44c9-b29c-192aac10a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index all emails folder enron_short/maildir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0ff0f-2711-46ad-8ec9-2bb422353b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for \"Norwegian and University and Science and Technology‚Äù and \"Norwegian University Science Technology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545c678-aa0f-4e8b-b8d3-a0ecd6c4e802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
