{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73722595-d5ca-4373-add0-b28e411a12ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: elasticsearch in /opt/conda/lib/python3.11/site-packages (8.10.1)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /opt/conda/lib/python3.11/site-packages (from elasticsearch) (8.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /opt/conda/lib/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch) (2.0.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from elastic-transport<9,>=8->elasticsearch) (2023.7.22)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Downloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, nltk\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.8.1 regex-2023.10.3\n"
     ]
    }
   ],
   "source": [
    "!pip install elasticsearch\n",
    "!pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ca2200-e0ce-4af8-bdfb-9532b17b5cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'es-node', 'cluster_name': 'tdt4117-ir-data-cluster', 'cluster_uuid': '7rryNDgDS1yAM3OPC7QvQA', 'version': {'number': '8.4.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '89f8c6d8429db93b816403ee75e5c270b43a940a', 'build_date': '2022-09-14T16:26:04.382547801Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "ELASTIC_PASSWORD = \"p2iFCHUbC7ze1QoIMVw\"\n",
    "\n",
    "es = Elasticsearch(\"http://elasticsearch:9200\",\n",
    "                    basic_auth=(\"elastic\", ELASTIC_PASSWORD))\n",
    "\n",
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f24de715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# b) Indexer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# b) Indexer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tokenize a document and give out a set without stopwords\n",
    "def tokenize(document): \n",
    "    #Just a list of all stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize or split into a list of words\n",
    "    word_tokens = word_tokenize(document);\n",
    "    token_set = set(word_tokens)\n",
    "\n",
    "    # Adds word to list as lowercase, if not in stop_words\n",
    "    filtered_sentence = [ w for w in token_set if not w.lower in stop_words ]\n",
    "\n",
    "    return filtered_sentence\n",
    "\n",
    "\n",
    "# Take list of documents and gives out tokenized list with the inverted index\n",
    "def build_inverted_index(documents):\n",
    "    inverted_index = defaultdict(list)\n",
    "    # Enumerate gives an index and the value\n",
    "    for doc_id, document in enumerate(documents):\n",
    "        tokens = tokenize(document)\n",
    "        \n",
    "        for token in tokens: \n",
    "            inverted_index[token].append(doc_id)\n",
    "    return inverted_index\n",
    "\n",
    "# Writes a file with the terms of the inverted indexes\n",
    "def save_index(index, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for term, postings in index.items():\n",
    "            file.write(f\"{term}: {', '.join(map(str, postings))}\\n\")\n",
    "\n",
    "def get_content(file_path: Path):\n",
    "    with file_path.open(mode='r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "                \n",
    "def get_path():\n",
    "    current_script_directory = os.path.dirname(os.path.abspath('elk_base_documents_template.ipynb'))\n",
    "    folder_name = \"DataAssignment4\"\n",
    "    path = os.path.join(current_script_directory, folder_name)\n",
    "    return path\n",
    "\n",
    "        \n",
    "def index():\n",
    "    path = get_path()\n",
    "    print(path)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = Path(os.path.join(root, file))\n",
    "                content = get_content(file_path)\n",
    "\n",
    "                # Tokenize and invert indexes\n",
    "                index = build_inverted_index(content)\n",
    "\n",
    "                # Save the indexes to a file for easy access\n",
    "                save_index(index, 'output.txt')\n",
    "\n",
    "                # Return the inverted index\n",
    "                return index\n",
    "            \n",
    "index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d40d0c-9401-41ab-85ba-01ffba121098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/DataAssignment4\n"
     ]
    }
   ],
   "source": [
    "# TODO import documents into elastic\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "index_name = \"a4\"\n",
    "\n",
    "def get_content(file_path: Path):\n",
    "    with file_path.open(mode='r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "\n",
    "def index_to_elasticsearch(content, doc_id):\n",
    "    # print(index_name, doc_id, content)\n",
    "    es.index(index=index_name, id=doc_id, document={'content': content})\n",
    "\n",
    "def get_path():\n",
    "    current_script_directory = os.path.dirname(os.path.abspath('elk_base_documents_template.ipynb'))\n",
    "    folder_name = \"DataAssignment4\"\n",
    "    path = os.path.join(current_script_directory, folder_name)\n",
    "    return path\n",
    "\n",
    "def add_to_elastic():\n",
    "    path = get_path()\n",
    "    print(path)\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = Path(os.path.join(root, file))\n",
    "                content = get_content(file_path)\n",
    "                # Use the file name as a unique ID for the document\n",
    "                doc_id = os.path.splitext(file)[0]\n",
    "                index_to_elasticsearch(content, doc_id)\n",
    "\n",
    "add_to_elastic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e8c5178-48dd-44c7-af26-8dea4116e45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 6 Hits:\n",
      "5.417975\n",
      "0.1597732\n",
      "0.15703472\n",
      "0.15641768\n",
      "0.15391785\n",
      "0.14406839\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d74303d-842c-4061-a4f9-024bdea1b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_script_directory = os.path.dirname(os.path.abspath('elk_base_documents_template.ipynb'))\n",
    "\n",
    "folder_name = \"DataAssignment4\"\n",
    "\n",
    "path = os.path.join(current_script_directory, folder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b137d904-a317-445f-b425-acecff90d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1 Hits:\n",
      "1.5510131\n"
     ]
    }
   ],
   "source": [
    "# TODO Query \"claim\"\n",
    "resp = es.get(index=index_name, id=\"Text1\")\n",
    "\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "resp = es.search(index=index_name, query={\"match\": {\"content\": {\"query\": \"claim\"}}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(hit[\"_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11541753-dd90-4839-bea9-2430e7442779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1 Hits:\n",
      "1.5510131\n"
     ]
    }
   ],
   "source": [
    "# TODO Query \"claim*\"\n",
    "resp = es.get(index=index_name, id=\"Text1\")\n",
    "\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "resp = es.search(index=index_name, query={\"match\": {\"content\": \"claim*\"}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(hit[\"_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfd121de-1c98-458e-8e71-83109d92f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 6 Hits:\n",
      "Text6 : 5.417975\n",
      "Text1 : 0.1597732\n",
      "Text4 : 0.15703472\n",
      "Text5 : 0.15641768\n",
      "Text2 : 0.15391785\n",
      "Text3 : 0.14406839\n"
     ]
    }
   ],
   "source": [
    "# TODO Query \"claims of duty\"\n",
    "resp = es.get(index=index_name, id=\"Text1\")\n",
    "\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "resp = es.search(index=index_name, query={\"match\": {\"content\": \"claims of duty\"}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(hit[\"_id\"],\":\", hit[\"_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "468012a9-b81e-4e5f-b383-90d8d5ac60d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 6 Hits:\n",
      "Text6 : 5.417975\n",
      "Text1 : 0.1597732\n",
      "Text4 : 0.15703472\n",
      "Text5 : 0.15641768\n",
      "Text2 : 0.15391785\n",
      "Text3 : 0.14406839\n"
     ]
    }
   ],
   "source": [
    "# TODO Query \"claims of duty\" in an alternative way\n",
    "resp = es.get(index=index_name, id=\"Text1\")\n",
    "\n",
    "es.indices.refresh(index=index_name)\n",
    "\n",
    "resp = es.search(index=index_name, query={\"match\": {\"content\": {\"query\": \"claims of duty\"}}})\n",
    "print(\"Got %d Hits:\" % resp['hits']['total']['value'])\n",
    "for hit in resp['hits']['hits']:\n",
    "    print(hit[\"_id\"],\":\", hit[\"_score\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
